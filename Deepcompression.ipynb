{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "You are asked to complete the following files:\n",
    "* **pruned_layers.py**, which contains the pruning of DNNs to reduce the storage of insignificant weight parameters with 2 methods: pruning by percentage and prune by standara deviation.\n",
    "* **train_util.py**, which includes the training process of DNNs with pruned connections.\n",
    "* **quantize.py**, which applies the quantization (weight sharing) part on the DNN to reduce the storage of weight parameters.\n",
    "* **huffman_coding.py**, which applies the Huffman coding onto the weight of DNNs to further compress the weight size.\n",
    "\n",
    "You are asked to submit the following files:\n",
    "* **net_before_pruning.pt**, which is the weight parameters before applying pruning on DNN weight parameters.\n",
    "* **net_after_pruning.pt**, which is the weight paramters after applying pruning on DNN weight parameters.\n",
    "* **net_after_quantization.pt**, which is the weight parameters after applying quantization (weight sharing) on DNN weight parameters.\n",
    "* **codebook_vgg16.npy**, which is the quantization codebook of each layer after applying quantization (weight sharing).\n",
    "* **huffman_encoding.npy**, which is the encoding map of each item within the quantization codebook in the whole DNN architecture.\n",
    "* **huffman_freq.npy**, which is the frequency map of each item within the quantization codebook in the whole DNN. \n",
    "\n",
    "To ensure fair grading policy, we fix the choice of model to VGG16_half, which is a down-scaled version of VGG16 using a width multiplier of 0.5. You may check the implementation in **vgg16.py** for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from vgg16 import VGG16, VGG16_half\n",
    "from train_util import *\n",
    "from quantize import quantize_whole_model\n",
    "from huffman_coding import huffman_coding\n",
    "from summary import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "from prune import prune\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-precision model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = VGG16_half()\n",
    "net = net.to(device)\n",
    "\n",
    "# Uncomment to load pretrained weights\n",
    "net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n",
    "\n",
    "# Comment if you have loaded pretrained weights\n",
    "# Tune the hyperparameters here.\n",
    "# train(net, epochs=75, batch_size=128, lr=0.01, reg=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3402, Test accuracy=0.9061\n"
     ]
    }
   ],
   "source": [
    "# Load the best weight paramters\n",
    "net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Summary before pruning-----\n",
      "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
      "1\t\tConvolutional\t864\t\t864\t\t\t0.000000\n",
      "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "4\t\tConvolutional\t9216\t\t9216\t\t\t0.000000\n",
      "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "7\t\tConvolutional\t18432\t\t18432\t\t\t0.000000\n",
      "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "10\t\tConvolutional\t36864\t\t36864\t\t\t0.000000\n",
      "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "13\t\tConvolutional\t73728\t\t73728\t\t\t0.000000\n",
      "14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "16\t\tConvolutional\t147456\t\t147456\t\t\t0.000000\n",
      "17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "19\t\tConvolutional\t147456\t\t147456\t\t\t0.000000\n",
      "20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "22\t\tConvolutional\t294912\t\t294912\t\t\t0.000000\n",
      "23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "25\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n",
      "26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "28\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n",
      "29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "31\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n",
      "32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "34\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n",
      "35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "37\t\tConvolutional\t589824\t\t589824\t\t\t0.000000\n",
      "38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "40\t\tLinear\t\t65536\t\t65536\t\t\t0.000000\n",
      "41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "43\t\tLinear\t\t65536\t\t65536\t\t\t0.000000\n",
      "44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "46\t\tLinear\t\t2560\t\t2560\t\t\t0.000000\n",
      "Total nonzero parameters: 3811680\n",
      "Total parameters: 3811680\n",
      "Total sparsity: 0.000000\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----Summary before pruning-----\")\n",
    "summary(net)\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning & Finetune with pruned connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=4.4284, Test accuracy=0.1000\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy before fine-tuning\n",
    "prune(net, method='percentage', q=80.0, s=0.75)\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment to load pretrained weights\n",
    "net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "# Comment if you have loaded pretrained weights\n",
    "# finetune_after_prune(net, epochs=50, batch_size=128, lr=0.001, reg=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.4094, Test accuracy=0.8862\n"
     ]
    }
   ],
   "source": [
    "# Load the best weight paramters\n",
    "net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Summary After pruning-----\n",
      "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
      "1\t\tConvolutional\t864\t\t172\t\t\t0.800926\n",
      "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "4\t\tConvolutional\t9216\t\t1843\t\t\t0.800022\n",
      "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "7\t\tConvolutional\t18432\t\t3686\t\t\t0.800022\n",
      "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "10\t\tConvolutional\t36864\t\t7372\t\t\t0.800022\n",
      "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "13\t\tConvolutional\t73728\t\t14745\t\t\t0.800008\n",
      "14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "16\t\tConvolutional\t147456\t\t29491\t\t\t0.800001\n",
      "17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "19\t\tConvolutional\t147456\t\t29491\t\t\t0.800001\n",
      "20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "22\t\tConvolutional\t294912\t\t58982\t\t\t0.800001\n",
      "23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "25\t\tConvolutional\t589824\t\t117964\t\t\t0.800001\n",
      "26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "28\t\tConvolutional\t589824\t\t117964\t\t\t0.800001\n",
      "29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "31\t\tConvolutional\t589824\t\t117964\t\t\t0.800001\n",
      "32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "34\t\tConvolutional\t589824\t\t117964\t\t\t0.800001\n",
      "35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "37\t\tConvolutional\t589824\t\t117964\t\t\t0.800001\n",
      "38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "40\t\tLinear\t\t65536\t\t13108\t\t\t0.799988\n",
      "41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "43\t\tLinear\t\t65536\t\t13108\t\t\t0.799988\n",
      "44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "46\t\tLinear\t\t2560\t\t512\t\t\t0.800000\n",
      "Total nonzero parameters: 762330\n",
      "Total parameters: 3811680\n",
      "Total sparsity: 0.800002\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----Summary After pruning-----\")\n",
    "summary(net)\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 1 layers quantization...\n",
      "Complete 2 layers quantization...\n",
      "Complete 3 layers quantization...\n",
      "Complete 4 layers quantization...\n",
      "Complete 5 layers quantization...\n",
      "Complete 6 layers quantization...\n",
      "Complete 7 layers quantization...\n",
      "Complete 8 layers quantization...\n",
      "Complete 9 layers quantization...\n",
      "Complete 10 layers quantization...\n",
      "Complete 11 layers quantization...\n",
      "Complete 12 layers quantization...\n",
      "Complete 13 layers quantization...\n",
      "Complete 14 layers quantization...\n",
      "Complete 15 layers quantization...\n",
      "Complete 16 layers quantization...\n"
     ]
    }
   ],
   "source": [
    "centers = quantize_whole_model(net, bits=4)\n",
    "np.save(\"codebook_vgg16.npy\", centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"net_after_pruning.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.4361, Test accuracy=0.8768\n"
     ]
    }
   ],
   "source": [
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6977 bits\n",
      "Complete 1 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.7005 bits\n",
      "Complete 2 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6503 bits\n",
      "Complete 3 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6987 bits\n",
      "Complete 4 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6995 bits\n",
      "Complete 5 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.7306 bits\n",
      "Complete 6 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.7060 bits\n",
      "Complete 7 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.7096 bits\n",
      "Complete 8 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6881 bits\n",
      "Complete 9 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6531 bits\n",
      "Complete 10 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6536 bits\n",
      "Complete 11 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.5749 bits\n",
      "Complete 12 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6095 bits\n",
      "Complete 13 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6656 bits\n",
      "Complete 14 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6714 bits\n",
      "Complete 15 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6836 bits\n",
      "Complete 16 layers for Huffman Coding...\n"
     ]
    }
   ],
   "source": [
    "frequency_map, encoding_map = huffman_coding(net, centers)\n",
    "np.save(\"huffman_encoding\", encoding_map)\n",
    "np.save(\"huffman_freq\", frequency_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-prune iterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment to load pretrained weights\n",
    "net.load_state_dict(torch.load(\"net_prune_train_iter.pt\"))\n",
    "# Comment if you have loaded pretrained weights\n",
    "# prune_train_iter(net, epochs=75, batch_size=128, lr=0.01, reg=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3633, Test accuracy=0.9061\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net_prune_train_iter.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3634, Test accuracy=0.9043\n"
     ]
    }
   ],
   "source": [
    "prune(net, method='percentage', q=80.0, s=0.75)\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment to load pretrained weights\n",
    "net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "# Comment if you have loaded pretrained weights\n",
    "# finetune_after_prune(net, epochs=100, batch_size=128, lr=0.001, reg=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3937, Test accuracy=0.9058\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Summary After pruning-----\n",
      "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
      "1\t\tConvolutional\t864\t\t172\t\t\t0.800926\n",
      "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "4\t\tConvolutional\t9216\t\t1843\t\t\t0.800022\n",
      "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "7\t\tConvolutional\t18432\t\t3686\t\t\t0.800022\n",
      "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "9\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "10\t\tConvolutional\t36864\t\t7372\t\t\t0.800022\n",
      "11\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "12\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "13\t\tConvolutional\t73728\t\t14745\t\t\t0.800008\n",
      "14\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "15\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "16\t\tConvolutional\t147456\t\t29491\t\t\t0.800001\n",
      "17\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "18\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "19\t\tConvolutional\t147456\t\t29491\t\t\t0.800001\n",
      "20\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "21\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "22\t\tConvolutional\t294912\t\t58982\t\t\t0.800001\n",
      "23\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "24\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "25\t\tConvolutional\t589824\t\t117964\t\t\t0.800001\n",
      "26\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "27\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "28\t\tConvolutional\t589824\t\t117964\t\t\t0.800001\n",
      "29\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "30\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "31\t\tConvolutional\t589824\t\t117964\t\t\t0.800001\n",
      "32\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "33\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "34\t\tConvolutional\t589824\t\t117964\t\t\t0.800001\n",
      "35\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "36\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "37\t\tConvolutional\t589824\t\t117964\t\t\t0.800001\n",
      "38\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "39\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "40\t\tLinear\t\t65536\t\t13108\t\t\t0.799988\n",
      "41\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "42\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "43\t\tLinear\t\t65536\t\t13108\t\t\t0.799988\n",
      "44\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "45\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "46\t\tLinear\t\t2560\t\t512\t\t\t0.800000\n",
      "Total nonzero parameters: 762330\n",
      "Total parameters: 3811680\n",
      "Total sparsity: 0.800002\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----Summary After pruning-----\")\n",
    "summary(net)\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 1 layers quantization...\n",
      "Complete 2 layers quantization...\n",
      "Complete 3 layers quantization...\n",
      "Complete 4 layers quantization...\n",
      "Complete 5 layers quantization...\n",
      "Complete 6 layers quantization...\n",
      "Complete 7 layers quantization...\n",
      "Complete 8 layers quantization...\n",
      "Complete 9 layers quantization...\n",
      "Complete 10 layers quantization...\n",
      "Complete 11 layers quantization...\n",
      "Complete 12 layers quantization...\n",
      "Complete 13 layers quantization...\n",
      "Complete 14 layers quantization...\n",
      "Complete 15 layers quantization...\n",
      "Complete 16 layers quantization...\n"
     ]
    }
   ],
   "source": [
    "centers = quantize_whole_model(net, bits=4)\n",
    "np.save(\"codebook_vgg16.npy\", centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"net_after_quantization.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.4002, Test accuracy=0.9009\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net_after_quantization.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.8023 bits\n",
      "Complete 1 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.7097 bits\n",
      "Complete 2 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.7718 bits\n",
      "Complete 3 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.8456 bits\n",
      "Complete 4 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.8743 bits\n",
      "Complete 5 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.8693 bits\n",
      "Complete 6 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.8490 bits\n",
      "Complete 7 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.8385 bits\n",
      "Complete 8 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.8206 bits\n",
      "Complete 9 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.8336 bits\n",
      "Complete 10 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6682 bits\n",
      "Complete 11 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.5771 bits\n",
      "Complete 12 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.4947 bits\n",
      "Complete 13 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.7260 bits\n",
      "Complete 14 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.8106 bits\n",
      "Complete 15 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.6895 bits\n",
      "Complete 16 layers for Huffman Coding...\n"
     ]
    }
   ],
   "source": [
    "frequency_map, encoding_map = huffman_coding(net, centers)\n",
    "np.save(\"huffman_encoding\", encoding_map)\n",
    "np.save(\"huffman_freq\", frequency_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
